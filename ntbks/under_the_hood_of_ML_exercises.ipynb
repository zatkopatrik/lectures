{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Preprocessing And Wrangling - Exercises <span style=\"color:red\">[EMPTY]</span></center>\n",
    "\n",
    "#### <center> Hint: Run the cells below until the `exercises` cell. Follow the description for the exercises. <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import Latex\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Generation for LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = 50\n",
    "m = 15\n",
    "n = 1\n",
    "x, y, coeff = sklearn.datasets.make_regression(\n",
    "        n_samples = m, \n",
    "        n_features = n,\n",
    "        noise = 40,\n",
    "        coef = True,\n",
    "        bias = bias,\n",
    "        random_state = 42 # if you have read Hitchiker's Guide to the Galaxy then\n",
    "                          # you know that 42 is the universal answer to life, the universe and everything\n",
    "                          # https://www.quora.com/Why-do-we-choose-random-state-as-42-very-often-during-training-a-machine-learning-model\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Coefficients of LR: \\n\\nBeta_0: {bias}\\nBeta_1: {np.round(coeff,2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\widehat{y} = \\beta_0 + \\beta_1 x_1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the scatter plot with a line that has an intercept bias and slope coeff\n",
    "plt.scatter(x,y)\n",
    "axes = plt.gca()\n",
    "x_vals = np.array(axes.get_xlim())\n",
    "y_vals = bias + coeff * x_vals\n",
    "plt.plot(x_vals, y_vals, 'r--')\n",
    "plt.title('Fit Line')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors(x, y, bias=bias, coeff=coeff, title1='Errors', title2='Squared Errors'):\n",
    "    \"\"\"\n",
    "    The function plots scatter of x, y and line with bias and coefficient. It also calculates RMSE.\n",
    "    ---------------\n",
    "    params:\n",
    "    - x: points on the x-axis\n",
    "    - y: points on the y-axis\n",
    "    - bias: intercept of the line\n",
    "    - coeff: slope of the line\n",
    "    - title1: title of the 1st chart\n",
    "    - title2: title of the 2nd chart\n",
    "    \"\"\"\n",
    "    y_hat = bias + x * coeff # predictions of x, we do not have intercept\n",
    "    diff = y - y_hat.T # differences between predictions and real values\n",
    "\n",
    "    # let's sort the values\n",
    "    x_sorted_indices = x.T.argsort() \n",
    "    x_sorted = x[x_sorted_indices].reshape(1,15)\n",
    "    diff_sorted = diff.T[x_sorted_indices].reshape(1,15)\n",
    "    diff_sorted_sq = diff_sorted**2\n",
    "    y_hat_sorted = y_hat[x_sorted_indices].reshape(1,15)\n",
    "\n",
    "    # x, y\n",
    "    plt.title(title1)\n",
    "    plt.scatter(x,y)\n",
    "    \n",
    "    # line\n",
    "    axes = plt.gca()\n",
    "    x_vals = np.array(axes.get_xlim())\n",
    "    y_vals = bias + coeff * x_vals\n",
    "    plt.plot(x_vals, y_vals, 'r--')\n",
    "    \n",
    "    # errors\n",
    "    plt.bar(x_sorted[0], diff_sorted[0], bottom = y_hat_sorted[0], width = 0.02, color = 'g')\n",
    "    plt.show()\n",
    "        \n",
    "    # RMSE\n",
    "    plt.title(title2)\n",
    "    plt.bar(x_sorted[0], diff_sorted_sq[0], width = 0.05, color = 'g')\n",
    "    MSE = np.mean(diff_sorted_sq)\n",
    "    plt.hlines(MSE, xmin=np.min(x_sorted[0]), xmax=np.max(x_sorted[0]))\n",
    "    plt.show()\n",
    "    print(f'Total average difference is: {round(MSE,1)}')\n",
    "    print(f'RMSE: {round(np.sqrt(MSE),1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^{n}{\\left(y_i-\\widehat{y_i}\\right)^2} $$\n",
    "$$RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}{\\left(y_i-\\widehat{y_i}\\right)^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"><center> INSTRUCTIONS: </center></span>\n",
    "\n",
    "1. Start your code in the editor after `### your code here###`. <br>\n",
    "2. You will have to write the code yourself if there is ### in the line. <br>\n",
    "3. If you see the `----` line, this is the place where you should and fill in your code.\n",
    "\n",
    "## <center> Exercise 1/a<center>\n",
    "\n",
    "test the `errors()` function with different values of bias and coeff\n",
    "\n",
    "try e.g.:\n",
    "- bias = 0, coeff = - 100\n",
    "- bias = -100, coeff = 0\n",
    "- bias = 200, coeff = 100\n",
    "- bias = 50, coeff = 61.9\n",
    "\n",
    "observe the behavior of the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here ###\n",
    "\n",
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_eq(x,y):\n",
    "    X = np.c_[np.ones(len(x)),x]\n",
    "    beta_hat = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "    return(beta_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Exercise 1/b<center>\n",
    "The function `normal_eq()`  computes $ \\widehat{\\beta}= (x^T x)^{-1} x^T y $.\n",
    "\n",
    "- Use the `normal_eq()` on input features x and output vector y. \n",
    "- Use the returned output as the values of bias and coeff in the `errors()` function. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here ###\n",
    "\n",
    "### \n",
    "### print(f'beta_0: {round(----,2)}\\nbeta_1: {round(----,2)}')\n",
    "\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <center>Exercise 1/c<center>\n",
    "- Fit LinearRegression on our x, y https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "- Return values of intercept and coefficient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here ###\n",
    "###\n",
    "###\n",
    "###\n",
    "### print(f'beta_0: {np.round(----,2)}\\nbeta_1: {np.round(----,2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$MSE = \\frac{1}{m} \\sum_{i=1}^{m}{\\left(y_i-\\widehat{y_i}\\right)^2} = \\frac{1}{m} \\sum_{i=1}^{m}{\\left(y_i-\\beta^T x_i\\right)^2} = \\frac{1}{m} \\left(y-\\beta^T x\\right)^2 = \\frac{1}{m} \\left(y-\\beta^T x\\right)^T \\left(y-\\beta^T x\\right) = \\frac{1}{m} \\left(y^T- (\\beta^T x)^T\\right) \\left(y-\\beta^T x\\right) = \\frac{1}{m} \\left(y^T y- y^T \\beta^T x - (\\beta^T x)^T y + (\\beta^T x)^T \\beta^T x \\right)$$\n",
    "\n",
    "$\\beta^T x$ is a vector and so is also y, therefore the order of the multiplication does not matter \n",
    "\n",
    "$$ J(\\beta) = \\frac{1}{m} \\left(y^T y -2 (\\beta^T x)^T y + \\beta^T x (\\beta^T x)^T \\right)$$\n",
    "\n",
    "We want to find a minimum of the cost function with respect to parameter $\\beta$ => $ \\frac{\\partial J(\\beta)}{\\partial \\beta} = 0$\n",
    "\n",
    "$$ \\frac{\\partial J(\\beta)}{\\partial \\beta} = -\\frac{2}{m} x^T y + \\frac{2}{m}  x^T x \\beta = 0$$\n",
    "\n",
    "$$  x^T x \\beta = x^T y $$\n",
    "\n",
    "\n",
    "\n",
    "$$  \\beta = (x^T x)^{-1} x^T y $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$ min_{\\beta} \\left[ MSE \\right] =  min_{\\beta} \\left[\\frac{1}{m} \\sum_{i=1}^{m} \\left( y_i-\\beta^T x_i\\right)^2 \\right] = min_{\\beta} \\left( J(\\beta) \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$MSE(\\beta) = \\frac{1}{m} \\left( y - \\beta^T X \\right)^2 $$\n",
    "\n",
    "\n",
    "$$\\nabla_{\\beta} MSE(\\beta) = - \\frac{2}{m}X^T(y - \\beta^T X) $$\n",
    "\n",
    "$$\\beta_{t+1} = \\beta_{t} - \\alpha \\nabla_{\\beta} MSE(\\beta)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$MSE(\\beta) = \\frac{1}{m} \\left( y - \\beta^T X \\right)^2 $$\n",
    "\n",
    "\n",
    "$$\\nabla_{\\beta} MSE(\\beta) = \\frac{2}{m}\\text{random_example }^T(\\text{random_example }\\beta - y) $$\n",
    "\n",
    "$$\\beta_{t+1} = \\beta_{t} - \\alpha \\nabla_{\\beta} MSE(\\beta)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, alpha=0.1, n_iter=100, return_betas=False):\n",
    "    \"\"\"\n",
    "    The function computes optimal parameters for linear regression model using gradient descent\n",
    "    ------------\n",
    "    params:\n",
    "        x: input features matrix\n",
    "        y: ground truth output vector\n",
    "        alpha: learning rate\n",
    "        n_iter: number of iterations\n",
    "    \"\"\"\n",
    "    X = np.c_[np.ones(len(x)),x]\n",
    "    m = X.shape[0] # #observations\n",
    "    n_betas = X.shape[1]\n",
    "    beta = np.random.randn(n_betas, 1)*200 # initialize randomly coefficients beta_0 and beta_1\n",
    "    y_ = y.copy().reshape(len(y), 1) # align dimension with X as y was previously of shape (15,). we want shape (15, 1)\n",
    "\n",
    "    betas_0 = []\n",
    "    betas_1 = []\n",
    "    for iterr in range(int(n_iter)):\n",
    "        gradient = 2/m*X.T.dot((X.dot(beta) - y_))\n",
    "        betas_0.append(beta[0][0])\n",
    "        betas_1.append(beta[1][0])\n",
    "        beta = beta - alpha * gradient\n",
    "    print(f\"Final parameters' value: {beta}\")\n",
    "    if return_betas:\n",
    "        return(betas_0, betas_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run gradient descent on our data learning parameters by minimizing MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas_0, betas_1 = gradient_descent(x, y, alpha=0.1, return_betas=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_MSE(x, y, beta1_range, bias = 44.46):\n",
    "    \"\"\"\n",
    "    compute value of MSE given different value of parameter beta_1 given in values beta1_range\n",
    "    \"\"\"\n",
    "    MSE = []\n",
    "    y = y.reshape(len(y), 1)\n",
    "\n",
    "    for coeff in beta1_range:\n",
    "        y_hat = bias + x * coeff\n",
    "        MSE.append(np.mean((y-y_hat)**2)) \n",
    "    return(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1_range = np.arange(-30, 171, 0.05)\n",
    "MSE = compute_MSE(x, y, beta1_range=b1_range)\n",
    "MSE_betas = compute_MSE(x, y, beta1_range=betas_1)\n",
    "\n",
    "plt.plot(b1_range, MSE)\n",
    "plt.title('MSE(coeff)')\n",
    "plt.xlabel('Coefficient')\n",
    "plt.ylabel('MSE')\n",
    "plt.scatter(betas_1, MSE_betas, color = 'r')\n",
    "plt.show()\n",
    "print(f'Minimum of the function is for beta_1 = {betas_1[-1]}\\nMSE in this point = {MSE_betas[-1]}\\nRMSE in this point = {np.sqrt(MSE_betas[-1])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal set of parameters was found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors(x,y,bias=betas_0[-1],coeff=betas_1[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gradient_descent(beta1_range, MSE, betas_1, MSE_betas):\n",
    "    plt.plot(beta1_range, MSE)\n",
    "    plt.title('MSE(coeff)')\n",
    "    plt.xlabel('Coefficient')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.plot(betas_1, MSE_betas, color = 'r')\n",
    "    plt.show()\n",
    "    print(f'Minimum of the function is for beta_1 = {betas_1_[-1]}\\nMSE in this point = {MSE_betas_[-1]}\\nRMSE in this point = {np.sqrt(MSE_betas_[-1])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try with different learning rate and number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <center>Exercise 2/a<center>\n",
    "\n",
    "Test different values of alpha and n_iter and observe on the chart what happens.\n",
    "- `alpha = 1, n_iter = 100` => run the cell multiple times\n",
    "- `alpha = 1e-5, n_iter = 1e6` => might take some time\n",
    "- `alpha = 1e-4, n_iter = 1e3` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here ###\n",
    "### betas_0_, betas_1_ = gradient_descent(x, y, alpha=----, n_iter =---- , return_betas=True) \n",
    "\n",
    "MSE = compute_MSE(x, y, beta1_range=np.arange(-130, 271, 0.05))\n",
    "MSE_betas_ = compute_MSE(x, y, beta1_range=betas_1_)\n",
    "\n",
    "plot_gradient_descent(np.arange(-130, 271, 0.05), MSE, betas_1_, MSE_betas_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ ||\\nabla_{\\beta} J(\\beta)||<\\epsilon$$\n",
    "$$ \\alpha = \\frac{\\alpha}{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic GD\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_lr = SGDRegressor()\n",
    "sgd_lr.fit(x, y)\n",
    "print(f'beta_0: {sgd_lr.intercept_}\\nbeta_1: {sgd_lr.coef_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "sgd_lr = SGDRegressor(max_iter=1e5, eta0 = alpha)\n",
    "sgd_lr.fit(x, y)\n",
    "print(f'beta_0: {sgd_lr.intercept_}\\nbeta_1: {sgd_lr.coef_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate data for regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_r = 50\n",
    "mr = 300\n",
    "nr = 200\n",
    "xr, yr, coeff_r = sklearn.datasets.make_regression(\n",
    "        n_samples = mr, \n",
    "        n_features = nr,\n",
    "        noise = 40,\n",
    "        coef = True,\n",
    "        bias = bias_r,\n",
    "        random_state = 42 \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ MSE(\\beta) = \\frac{1}{m} \\sum_{1}^{m} \\left( y_i - \\widehat{y_i}\\right)^2 = \\frac{1}{m} \\left( y - \\beta^T X \\right)^2 $$\n",
    "$$ J(\\beta) = MSE(\\beta) + \\alpha \\sum_{1}^{n} \\beta_i^2 = MSE(\\beta) + \\alpha ||\\beta||_2^2$$\n",
    "$$ ||x||_2 = \\sqrt{x_1^2 + ... + x_n^2}$$\n",
    "\n",
    "$$ \\beta = \\left(X_T X + \\alpha I\\right)^{-1}X^T Y $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coef(lr_coef):\n",
    "    '''\n",
    "    The function plots coefficients' values from the linear model.\n",
    "    --------\n",
    "    params:\n",
    "        lr_coef: coefficients as they are returned from the classifier's attributes\n",
    "    '''\n",
    "    print(f'AVG coef value: {np.mean(lr_coef)}')\n",
    "    plt.plot(lr_coef)\n",
    "    plt.title(\"Coefficients' values\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit linear regression without regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(xr, yr)\n",
    "plot_coef(lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <center>Exercise 3/a<center>\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
    "\n",
    "fit Ridge and Lasso regression on (xr, yr). Test different values of alpha and interpret from the chart what happens. \n",
    "- `alpha = 20` \n",
    "- `alpha = 75` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here ###\n",
    "###\n",
    "###\n",
    "###\n",
    "plot_coef(lr_r.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ J(\\beta) = MSE(\\beta) + \\alpha \\sum_{1}^{n} |\\beta_i| = MSE(\\beta) + \\alpha ||\\beta||_1$$\n",
    "\n",
    "$$ ||x||_1 = |x_1| + ... + |x_n| $$\n",
    "\n",
    "$$ ||x||_p = \\left(|x_1|^p + ... + |x_n|^p\\right)^{(1/p)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <center>Exercise 3/b<center>\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\n",
    "\n",
    "fit Lasso regression on (xr, yr). Test different values of alpha and interpret from the chart what happens. \n",
    "\n",
    "- `alpha = 20` \n",
    "- `alpha = 75` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### your code here ###\n",
    "###\n",
    "###\n",
    "###\n",
    "plot_coef(lr_l.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ J(\\beta) = MSE(\\beta) + \\alpha ||\\beta||_1 + (1-\\alpha) ||\\beta||_2^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "lr_en = ElasticNet(l1_ratio=1, alpha=75)\n",
    "lr_en.fit(xr, yr)\n",
    "plot_coef(lr_en.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation For Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "x_log, y_log = make_blobs(n_samples=100, centers=2, n_features=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x_s = np.arange(np.min(x_log),np.max(x_log),0.5)\n",
    "sig = sigmoid(x_s)\n",
    "plt.plot(x_s, sig, c='k')\n",
    "plt.scatter(x_log, y_log)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('sigmoid function')\n",
    "plt.hlines(0.5, np.min(x_log), np.max(x_log), 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "x_log, y_log = make_blobs(n_samples=100, centers=5, n_features=10, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_log, y_log, random_state=42)\n",
    "logr= LogisticRegression()\n",
    "logr.fit(X_train, y_train)\n",
    "confusion_matrix(logr.predict(X_test), y_test)\n",
    "# logr.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n$$\n",
    "\n",
    "$$ \\sigma\\left(\\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\widehat{p} = \\sigma(x) = \\frac{1}{1+exp(-x)}$$\n",
    "\n",
    "$$ logit(p) = log(\\frac{p}{1-p}) = log\\left(\\frac{\\frac{1}{1+exp(-x)}}{1-\\frac{1}{1+exp(-x)}}\\right) = log\\left(\\frac{\\frac{1}{1+exp(-x)}}{\\frac{1 + exp(-x) - 1} {1+exp(-x)}}\\right) = log\\left(\\frac{1}{exp(-x)}\\right) = log(exp(x))= x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_s = np.arange(0.01,1.1,0.05)\n",
    "plt.plot(x_s, -np.log(x_s))\n",
    "plt.xlabel('p')\n",
    "plt.ylabel('f(p)')\n",
    "plt.title('-log(p)')\n",
    "plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "plt.tick_params(axis='both', which='minor', labelsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_s = np.arange(0.01,1.1,0.05)\n",
    "plt.plot(x_s, -np.log(1-x_s))\n",
    "plt.xlabel('p')\n",
    "plt.ylabel('f(p)')\n",
    "plt.title('-log(p)')\n",
    "plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "plt.tick_params(axis='both', which='minor', labelsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ C_1(\\beta) = -log(\\widehat{p}) $$\n",
    "\n",
    "$$ C_0(\\beta) = -log(1-\\widehat{p}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ J(\\beta) = \\frac{1}{m} \\sum_{1}^{m} \\left[ y_i C_1(\\beta) + (1 - y_i) C_0(\\beta)\\right] = \\frac{1}{m} \\sum_{1}^{m} \\left[ y_i (-log(\\widehat{p_i})) + (1 - y_i) (-log(1 - \\widehat{p_i}))\\right]$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
